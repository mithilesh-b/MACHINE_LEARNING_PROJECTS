{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translator import utils\n",
    "from translator import train\n",
    "from translator import models\n",
    "from translator import datasets\n",
    "from translator import config as cfg\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = 'data/ben-eng/ben.txt'\n",
    "text_data = datasets.TatoebaDataset(path_to_dataset, cfg.NUM_DATA_TO_LOAD)\n",
    "    \n",
    "# retrive data and tokenizers\n",
    "tensors, tokenizer = text_data.load_data()\n",
    "input_tensor, target_tensor = tensors \n",
    "inp_lang_tokenizer, targ_lang_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer write at: models/input_language_tokenizer.json\n",
      "Tokenizer write at: models/target_language_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# save tokenizer for further use\n",
    "utils.save_tokenizer(\n",
    "    tokenizer=inp_lang_tokenizer,\n",
    "    save_at='models',\n",
    "    file_name='input_language_tokenizer.json')\n",
    "utils.save_tokenizer(\n",
    "    tokenizer=targ_lang_tokenizer,\n",
    "    save_at='models',\n",
    "    file_name='target_language_tokenizer.json')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_train, input_val, target_train, target_val = \\\n",
    "    train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training params\n",
    "buffer_size = len(input_train)\n",
    "steps_per_epoch = len(input_train) // cfg.BATCH_SIZE\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index) + 1\n",
    "\n",
    "# convert data to tf.data formate\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train))\n",
    "dataset = dataset.shuffle(buffer_size)\n",
    "dataset = dataset.batch(cfg.BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init encoder & decoder\n",
    "encoder = models.Encoder(\n",
    "    vocab_inp_size, cfg.EMBEDDING_DIM, cfg.UNITS, cfg.BATCH_SIZE)\n",
    "decoder = models.Decoder(\n",
    "    vocab_tar_size, cfg.EMBEDDING_DIM, cfg.UNITS, cfg.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# init checkpoint \n",
    "checkpoint_dir = 'models/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.RESTORE_SAVED_CHECKPOINT:\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(cfg.EPOCHS):\n",
    "    print(\"Epoch {} / {}\".format(epoch, cfg.EPOCHS))\n",
    "    pbar = tqdm(dataset.take(steps_per_epoch), ascii=True, total=steps_per_epoch)\n",
    "\n",
    "    total_loss = 0\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "    for step, data in enumerate(pbar):\n",
    "        inp, targ = data\n",
    "        \n",
    "        batch_loss = train.train_step(\n",
    "            inp, targ, targ_lang_tokenizer,\n",
    "            enc_hidden, encoder, decoder, optimizer\n",
    "        )\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        pbar.set_description(\n",
    "            \"Step - {} / {} - batch loss - {:.4f} \"\n",
    "                .format(step+1, steps_per_epoch, batch_loss.numpy()))\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print('Epoch loss - {:.4f}'.format(total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
